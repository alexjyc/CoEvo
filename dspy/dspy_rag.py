"""
Self-Improving RAG Pipeline using LangGraph and RAGAS
Implements iterative refinement based on evaluation feedback
"""

import asyncio
from typing import Optional, List, Dict, Any, Annotated, Union, Tuple, Callable
from typing_extensions import TypedDict as ExtendedTypedDict

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from preprocessor import DocumentPreprocessor
from retrieval import Retriever, RetrievalMethod
from reranker import Reranker
from evaluation import Evaluator
from dspy_optimization import (
    DSPYPromptOrchestrator,
    PromptCallResult,
    RewardManager,
    optimize_prompts_with_representatives,
    build_trainsets_from_results,
)


def _bounded_history(max_items: int):
    """Merge function for LangGraph state that keeps only the most recent N entries."""
    def _merge(left, right):
        combined = (left or []) + (right or [])
        if not combined:
            return []
        return combined[-max_items:]
    return _merge


class RAGState(ExtendedTypedDict):
    """State schema for the self-improving RAG pipeline"""
    # Input
    query: str
    ground_truth: Optional[str]
    
    # Reformation
    reformulated_query: Optional[str]
    reformation_rationale: Optional[str]
    
    # Retrieval
    retrieved_chunks: Optional[List[Dict[str, Any]]]
    contexts: Optional[List[str]]
    retrieval_method: Optional[str]
    
    # Reranking
    reranked_chunks: Optional[List[Dict[str, Any]]]
    final_contexts: Optional[List[str]]
    reranking_rationale: Optional[str]
    
    # Generation
    answer: Optional[str]
    answer_reference: Optional[str]
    generation_rationale: Optional[str]
    
    # Evaluation (RAGAS scores)
    faithfulness_score: Optional[float]
    answer_relevancy_score: Optional[float]
    context_utilization_score: Optional[float]
    context_precision_score: Optional[float]
    context_recall_score: Optional[float]
    retrieval_eval: Optional[Dict[str, float]]
    reranking_eval: Optional[Dict[str, float]]
    generation_eval: Optional[Dict[str, float]]
    judge_eval: Optional[Dict[str, Dict[str, Any]]]
    overall_score: Optional[float]
    reformulation_feedback: Optional[str]

    # Iteration control
    iteration: int
    max_iterations: int
    converged: bool
    improvement_history: Annotated[List[Dict[str, Any]], _bounded_history(20)]
    prompt_history: Annotated[List[Dict[str, Any]], _bounded_history(20)]
    
    # Configuration
    retrieval_k: int
    final_k: int
    convergence_threshold: float
    retrieval_round: int
    rerank_round: int
    generation_round: int


class DSPYRAG:
    """
    Self-improving RAG Pipeline using LangGraph
    
    This pipeline iteratively refines its outputs based on RAGAS evaluation scores,
    adjusting retrieval parameters and query reformulation strategies.
    """
    
    def __init__(
        self,
        chunk_size: int = 600,
        chunk_overlap: int = 50,
        generation_model: str = "gpt-4o-mini",
        evaluator_model: str = "gpt-4o-mini",
        temperature: float = 0,
        retrieval_method: str = "hybrid",
        max_iterations: int = 3,
        convergence_threshold: float = 0.85,
        dspy_prompts: Optional[Dict[str, Any]] = None,
        enable_prompt_judge: bool = True,
        dspy_judge_prompts: Optional[Dict[str, str]] = None,
        reward_judge_weight: float = 0.1,
    ):
        """
        Initialize the self-improving RAG pipeline
        
        Args:
            openai_api_key: OpenAI API key
            chunk_size: Size of text chunks for processing
            chunk_overlap: Overlap between chunks
            generation_model: Model for generation
            evaluator_model: Model for RAGAS evaluation
            temperature: Generation temperature
            retrieval_method: "hybrid", "bm25", or "dense"
            max_iterations: Maximum refinement iterations
            convergence_threshold: Score threshold to stop iterations (0-1)
        """
        
        self.max_iterations = max_iterations
        self.convergence_threshold = convergence_threshold
        
        # Determine retrieval method
        if retrieval_method == "hybrid":
            self.retrieval_method = RetrievalMethod.HYBRID
        elif retrieval_method == "bm25":
            self.retrieval_method = RetrievalMethod.BM25_ONLY
        elif retrieval_method == "dense":
            self.retrieval_method = RetrievalMethod.DENSE_ONLY
        else:
            raise ValueError(f"Invalid retrieval method: {retrieval_method}")
        
        # Initialize components
        self.preprocessor = DocumentPreprocessor(chunk_size, chunk_overlap)
        self.retriever = None
        self.reranker = Reranker(default_k=10)
        self.evaluator = Evaluator(model=evaluator_model)
        self.prompter = DSPYPromptOrchestrator(
            enable_judge=enable_prompt_judge,
        )
        self.reward_manager = RewardManager(judge_weight=reward_judge_weight)
        
        self.is_ready = False
        
        # Build the LangGraph workflow
        self.graph = self._build_graph()
    
    def load_and_process_documents(
        self,
        documents: List[Dict[str, Any]],
        use_context: bool = True,
        *,
        evaluation_queries: Optional[List[Dict[str, Any]]] = None,
        rewrite_provider: Optional[Callable[[Dict[str, Any]], str]] = None,
        metric_fns: Optional[Dict[str, Callable[[Any, Any], float]]] = None,
    ):
        """
        Load documents, build indices, and optionally compile DSPy prompts using representatives.
        
        Args:
            documents: Corpus to ingest.
            use_context: Whether to run contextual retrieval during preprocessing.
            evaluation_queries: Seed queries (with optional ground_truth) used to pick representatives.
            rewrite_provider: Optional function that supplies target rewrites per representative.
            metric_fns: Optional per-stage metric functions for DSPy optimizers.
        """
        print("Loading and processing documents...")
        self.preprocessor.process_documents(documents, use_context=use_context)
        self.retriever = Retriever(self.preprocessor)
        self.is_ready = True
        print(f"Pipeline ready! Processed {len(documents)} documents.")
        
        if evaluation_queries:
            self._bootstrap_prompts_from_representatives(
                evaluation_queries=evaluation_queries,
                rewrite_provider=rewrite_provider,
                metric_fns=metric_fns,
            )
    
    def _bootstrap_prompts_from_representatives(
        self,
        *,
        evaluation_queries: List[Dict[str, Any]],
        rewrite_provider: Optional[Callable[[Dict[str, Any]], str]] = None,
        metric_fns: Optional[Dict[str, Callable[[Any, Any], float]]] = None,
    ) -> None:
        """Select representatives, compile DSPy modules, and cache the trainsets."""
        if not self.retriever:
            raise ValueError("Retriever not initialized; call load_and_process_documents first.")
        representatives = self.preprocessor.get_representative_data(evaluation_queries)
        if not representatives:
            print("No representative queries available for DSPy prompt optimization.")
            return
        
        def context_provider(rep: Dict[str, Any]) -> List[str]:
            return self._representative_contexts(rep, k=self.reranker.default_k)
        
        optimize_prompts_with_representatives(
            orchestrator=self.prompter,
            representatives=representatives,
            context_provider=context_provider,
            rewrite_provider=rewrite_provider,
            metric_fns=metric_fns,
        )
        print(f"Optimized DSPy prompts using {len(representatives)} representative queries.")
    
    def _representative_contexts(self, rep: Dict[str, Any], k: int) -> List[str]:
        """Retrieve top-k chunk texts for a representative query."""
        query = rep.get("query", "")
        if not query or not self.retriever:
            return []
        retrieval_result = self.retriever.retrieve(
            query,
            max(k, self.reranker.default_k),
            method=self.retrieval_method
        )
        if isinstance(retrieval_result, dict):
            combined = (retrieval_result.get("bm25") or []) + (retrieval_result.get("dense") or [])
        else:
            combined = retrieval_result or []
        if not combined:
            return []
        processed = self.reranker.process_chunks(combined, k=k)
        return [chunk["chunk_text"] for chunk in processed]

    def _refresh_prompts_from_results(
        self,
        results: List[Dict[str, Any]],
        *,
        threshold: float,
        window: int,
        metric_fns: Optional[Dict[str, Callable[[Any, Any], float]]] = None,
    ) -> bool:
        """Refresh DSPy modules between batches when rewards lag."""
        if not results:
            return False

        reward_values: List[float] = []
        for res in results:
            if not isinstance(res, dict):
                continue
            for entry in res.get("improvement_history") or []:
                reward = entry.get("reward")
                if isinstance(reward, (int, float)):
                    reward_values.append(float(reward))

        if not reward_values:
            print("No rewards available for prompt refresh; skipping.")
            return False

        w = max(window, 1)
        recent_rewards = reward_values[-w:]
        mean_reward = sum(recent_rewards) / len(recent_rewards)

        if mean_reward >= threshold:
            print(f"Prompt refresh skipped (mean reward={mean_reward:.3f} >= threshold={threshold:.2f}).")
            return False

        trainsets = build_trainsets_from_results(results)
        total_examples = sum(len(v) for v in trainsets.values())
        if total_examples == 0:
            print("No training examples derived from batch; skipping prompt refresh.")
            return False

        prev_version = getattr(self.prompter, "version", 0)
        self.prompter.compile(trainsets, metric_fns=metric_fns)
        new_version = getattr(self.prompter, "version", prev_version)
        print(
            f"Refreshed DSPy prompts from batch rewards (v{prev_version} -> v{new_version}); "
            f"mean reward={mean_reward:.3f}, examples={total_examples}"
        )
        return True
    
    def _build_graph(self) -> StateGraph:
        """Build the LangGraph workflow with feedback loops"""
        
        # Define the state graph
        graph = StateGraph(RAGState)
        
        # Add nodes
        graph.add_node("initialize", self._initialize_node)
        graph.add_node("reformulate_query", self._reformulate_query_node)
        graph.add_node("retrieve", self._retrieve_node)
        graph.add_node("evaluate_retrieval", self._evaluate_retrieval_node)
        graph.add_node("rerank", self._rerank_node)
        graph.add_node("evaluate_reranking", self._evaluate_reranking_node)
        graph.add_node("generate", self._generate_node)
        graph.add_node("evaluate_generation", self._evaluate_generation_node)
        
        # Add edges
        graph.add_edge(START, "initialize")
        graph.add_edge("initialize", "reformulate_query")
        graph.add_edge("reformulate_query", "retrieve")
        graph.add_edge("retrieve", "evaluate_retrieval")
        graph.add_edge("rerank", "evaluate_reranking")
        graph.add_edge("generate", "evaluate_generation")

        graph.add_conditional_edges(
            "evaluate_retrieval",
            self._route_after_retrieval_eval,
            {
                "reformulate_query": "reformulate_query",
                "rerank": "rerank"
            }
        )
        graph.add_conditional_edges(
            "evaluate_reranking",
            self._route_after_reranking_eval,
            {
                "rerank": "rerank",
                "generate": "generate"
            }
        )
        graph.add_conditional_edges(
            "evaluate_generation",
            self._route_after_generation_eval,
            {
                "generate": "generate",
                END: END
            }
        )
        
        return graph
    
    def _initialize_node(self, state: RAGState) -> Dict[str, Any]:
        """Initialize the iteration state"""
        return {
            "iteration": 0,
            "converged": False,
            "improvement_history": [],
            "max_iterations": state.get("max_iterations", self.max_iterations),
            "convergence_threshold": state.get("convergence_threshold", self.convergence_threshold),
            "retrieval_round": 0,
            "rerank_round": 0,
            "generation_round": 0,
            "reformulation_feedback": None,
        }
    
    def _reformulate_query_node(self, state: RAGState) -> Dict[str, Any]:
        """Reformulate the query, potentially adjusting based on previous feedback"""
        print(f"\n[Iteration {state['iteration']}] Reformulating query...")
        
        query = state["query"]
        print(f"  Original Query: {query}")
        feedback = state.get("reformulation_feedback") or ""
        result: PromptCallResult = self.prompter.reformulate(
            query,
            feedback=feedback,
        )
        rewritten = result.output["reformulated_query"]
        print(f"  Reformulated Query: {rewritten}")
        if feedback:
            print(f"  Reformulation feedback: {feedback}")

        prompt_record = self._augment_prompt_record(result.prompt_record, state)

        return {
            "reformulated_query": rewritten,
            "prompt_history": [prompt_record]
        }
    
    def _retrieve_node(self, state: RAGState) -> Dict[str, Any]:
        """Retrieve documents, potentially adjusting k based on feedback"""
        print(f"[Iteration {state['iteration']}] Retrieving documents...")
        
        effective_query = state["reformulated_query"]
        retrieval_k = state["retrieval_k"]
        
        # Perform retrieval
        retrieval_result = self.retriever.retrieve(
            effective_query,
            retrieval_k,
            method=self.retrieval_method
        )
        
        # Handle hybrid vs single method results
        # TODO: additional metrics for hybrid retrieval to measure fusion quality
        if isinstance(retrieval_result, dict) and self.retrieval_method == RetrievalMethod.HYBRID:
            retrieved_chunks = self.reranker.optimize_fusion_params(
                bm25_results=retrieval_result.get('bm25', []),
                dense_results=retrieval_result.get('dense', []),
                top_k=retrieval_k,
                quality_fn=self._compute_retrieval_quality
            )
        else:
            retrieved_chunks = retrieval_result
        
        contexts = [chunk['chunk_text'] for chunk in retrieved_chunks]
        
        return {
            "retrieved_chunks": retrieved_chunks,
            "contexts": contexts,
            "retrieval_method": self.retrieval_method.value
        }

    def _compute_retrieval_quality(self, fused_chunks: List[Dict[str, Any]]) -> Dict[str, float]:
        """Lightweight metrics to pick the best hybrid fusion weight."""
        if not fused_chunks:
            return {
                "fusion_quality": 0.0,
                "overlap_ratio": 0.0,
                "source_balance": 0.0,
                "score_concentration": 0.0,
                "rank_agreement": 0.0,
            }

        total = len(fused_chunks)
        overlap = bm25_only = dense_only = 0
        rank_agreements: List[float] = []
        fusion_scores: List[float] = []

        for chunk in fused_chunks:
            sources = chunk.get("hybrid_sources") or {}
            from_bm25 = bool(sources.get("bm25"))
            from_dense = bool(sources.get("dense"))

            if from_bm25 and from_dense:
                overlap += 1
            elif from_bm25:
                bm25_only += 1
            elif from_dense:
                dense_only += 1

            bm25_rank = chunk.get("bm25_rank")
            dense_rank = chunk.get("dense_rank")
            rank_parts: List[float] = []
            if isinstance(bm25_rank, (int, float)) and bm25_rank > 0:
                rank_parts.append(1.0 / float(bm25_rank))
            if isinstance(dense_rank, (int, float)) and dense_rank > 0:
                rank_parts.append(1.0 / float(dense_rank))
            if rank_parts:
                rank_agreements.append(sum(rank_parts) / len(rank_parts))

            fusion_scores.append(float(chunk.get("fusion_raw_score", 0.0)))

        overlap_ratio = overlap / total
        source_balance = 1.0 - abs(bm25_only - dense_only) / total
        source_balance = max(source_balance, 0.0)

        score_concentration = 0.0
        if fusion_scores:
            sorted_scores = sorted(fusion_scores, reverse=True)
            total_score = sum(sorted_scores) or 1e-9
            score_concentration = sum(sorted_scores[: min(3, len(sorted_scores))]) / total_score

        rank_agreement = sum(rank_agreements) / len(rank_agreements) if rank_agreements else 0.0

        fusion_quality = (
            0.4 * overlap_ratio
            + 0.25 * source_balance
            + 0.2 * score_concentration
            + 0.15 * rank_agreement
        )

        return {
            "fusion_quality": fusion_quality,
            "overlap_ratio": overlap_ratio,
            "source_balance": source_balance,
            "score_concentration": score_concentration,
            "rank_agreement": rank_agreement,
        }
    
    def _rerank_node(self, state: RAGState) -> Dict[str, Any]:
        """Rerank retrieved documents"""
        print(f"[Iteration {state['iteration']}] Reranking documents...")
        
        final_k = state["final_k"]
        retrieved_chunks = state.get("retrieved_chunks") or []
        
        processed_chunks = self.reranker.process_chunks(
            retrieved_chunks,
            k=final_k
        )
        
        final_contexts = [chunk['chunk_text'] for chunk in processed_chunks]
        prompt_history: List[Dict[str, Any]] = []
        reranking_rationale = None

        if final_contexts:
            result: PromptCallResult = self.prompter.rerank(state["query"], final_contexts)
            ranked_texts = result.output.get("ranked_contexts", final_contexts)
            reranking_rationale = result.output.get("rationale")
            ordered_chunks: List[Dict[str, Any]] = []
            remaining = processed_chunks.copy()
            for text in ranked_texts:
                match = next((chunk for chunk in remaining if chunk["chunk_text"] == text), None)
                if match:
                    ordered_chunks.append(match)
                    remaining.remove(match)
            ordered_chunks.extend(remaining)
            processed_chunks = ordered_chunks[:final_k]
            final_contexts = [chunk["chunk_text"] for chunk in processed_chunks]

            prompt_record = self._augment_prompt_record(result.prompt_record, state)
            prompt_history.append(prompt_record)
        else:
            reranking_rationale = None

        return {
            "reranked_chunks": processed_chunks,
            "final_contexts": final_contexts,
            "reranking_rationale": reranking_rationale,
            "prompt_history": prompt_history,
        }
    
    def _generate_node(self, state: RAGState) -> Dict[str, Any]:
        """Generate answer from contexts"""
        print(f"[Iteration {state['iteration']}] Generating answer...")
        contexts_to_use = state.get("final_contexts") or []
        result: PromptCallResult = self.prompter.generate(
            query=state["reformulated_query"],
            context=contexts_to_use,
            reference=state.get("ground_truth"),
        )
        prompt_record = self._augment_prompt_record(result.prompt_record, state)
        updates: Dict[str, Any] = {
            "answer": result.output.get("answer"),
            "answer_reference": None,
            "generation_rationale": result.output.get("rationale"),
            "prompt_history": [prompt_record],
        }
        return updates

    def _augment_prompt_record(self, prompt_record: Dict[str, Any], state: RAGState) -> Dict[str, Any]:
        """Attach iteration metadata to prompt records for later auditing."""
        record = dict(prompt_record)
        record.setdefault("stage", prompt_record.get("stage"))
        record["iteration"] = state.get("iteration", 0)
        return record

    def _stage_memory(self, stage: str, state_snapshot: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Stage-specific LangGraph memory for agentic rewards."""
        history = state_snapshot.get("improvement_history") or []
        return [entry for entry in history if entry.get("stage") == stage]

    def _latest_reward(self, state: RAGState, stage: str) -> Optional[float]:
        """Fetch the most recent reward for the given stage."""
        for entry in reversed(state.get("improvement_history") or []):
            if entry.get("stage") == stage and entry.get("reward") is not None:
                try:
                    return float(entry["reward"])
                except (TypeError, ValueError):
                    return None
        return None

    def _update_judge_eval(self, state: RAGState, stage: str, feedback: Dict[str, Any]) -> Dict[str, Any]:
        """Persist judge feedback in state for later reward/analysis use."""
        if not feedback:
            return {}
        judge_map = dict(state.get("judge_eval") or {})
        judge_map[stage] = feedback
        return {"judge_eval": judge_map}

    def _finalize_prompt_stage(
        self,
        stage: str,
        state_snapshot: Dict[str, Any],
        metric_overrides: Optional[Dict[str, Any]] = None
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Finalize RL updates for a specific stage with optional metric overrides."""
        snapshot = dict(state_snapshot)
        if metric_overrides:
            snapshot.update(metric_overrides)
        stage_memory = self._stage_memory(stage, snapshot)
        reward, breakdown = self.reward_manager.compute(stage, snapshot, stage_memory)
        prompt_history = snapshot.get("prompt_history") or []
        latest_prompt = next((p for p in reversed(prompt_history) if p.get("stage") == stage), {})
        prompt_record = {
            "stage": stage,
            "iteration": snapshot.get("iteration", 0),
            "prompt_name": latest_prompt.get("prompt_name"),
            "prompt_text": latest_prompt.get("prompt_text"),
            "reward": reward,
            "metrics_snapshot": metric_overrides or {},
            "reward_breakdown": breakdown,
        }
        improvements = [{
            "stage": stage,
            "iteration": snapshot.get("iteration", 0),
            "prompt_name": latest_prompt.get("prompt_name"),
            "reward": reward,
            "metrics_snapshot": metric_overrides or {},
            "reward_breakdown": breakdown,
        }]
        print(f"  Prompt updates ({stage}): reward={reward:.3f} total={breakdown.get('total', reward):.3f}")
        return [prompt_record], improvements
    
    async def _evaluate_retrieval_node(self, state: RAGState) -> Dict[str, Any]:
        """Evaluate retrieval quality before reranking."""
        print(f"[Iteration {state['iteration']}] Evaluating retrieval step...")
    
        retrieved_chunks = state.get("retrieved_chunks") or []
        
        if not retrieved_chunks:
            print("  No contexts retrieved, skipping retrieval evaluation.")
            round_idx = state.get("retrieval_round", 0) + 1
            return {"retrieval_eval": None, "retrieval_round": round_idx}
        
        # Use RAGAS to evaluate retrieval (context_precision & context_recall)
        contexts = [chunk['chunk_text'] for chunk in retrieved_chunks]
        print(f"  Query: {state['query']}")
        print(f"  Contexts: {contexts}")
        print(f"  Ground Truth: {state.get('ground_truth')}")
        scores = await self.evaluator.evaluate_retrieval(
            query=state["query"],
            contexts=contexts,
            ground_truth=state.get("ground_truth")
        )  
        
        print(f"  Retrieval RAGAS metrics: precision={scores.get('context_precision', 0.0):.3f}, "
              f"recall={scores.get('context_recall', 0.0):.3f}")
        
        round_idx = state.get("retrieval_round", 0) + 1

        feedback = None
        precision = scores.get("context_precision", 0.0)
        recall = scores.get("context_recall", 0.0)
        # if precision < 0.6 or recall < 0.6:
        #     import re

        #     q_terms = set(re.findall(r"[A-Z][A-Za-z0-9\-]+", state["query"]))
        #     ctx_blob = " ".join(contexts)
        #     missing = [t for t in q_terms if t not in ctx_blob]
        #     parts = [f"precision={precision:.2f}", f"recall={recall:.2f}"]
        #     if missing:
        #         parts.append(f"missing: {', '.join(missing[:5])}")
        #     feedback = "; ".join(parts)
        
        judge_update: Dict[str, Any] = {}
        if getattr(self.prompter, "judge", None) and getattr(self.prompter.judge, "enabled", False):
            judge_feedback = self.prompter.judge.assess_reformulation(
                query=state["query"],
                rewritten=state.get("reformulated_query", ""),
            )
            judge_update = self._update_judge_eval(state, "reformulation", judge_feedback)

        prompt_records, improvements = self._finalize_prompt_stage(
            stage="reformulation",
            state_snapshot=state,
            metric_overrides={
                "retrieval_eval": scores,
                "context_precision_score": scores.get('context_precision'),
                "context_recall_score": scores.get('context_recall')
            }
        )
        
        return {
            "retrieval_eval": scores,
            "retrieval_round": round_idx,
            "iteration": state.get("iteration", 0) + 1,
            "prompt_history": prompt_records or [],
            "improvement_history": improvements or [],
            "reformulation_feedback": feedback,
            **judge_update,
        }

    async def _evaluate_reranking_node(self, state: RAGState) -> Dict[str, Any]:
        """Evaluate reranking quality using score comparison (no LLM calls!)"""
        print(f"[Iteration {state['iteration']}] Evaluating reranking step...")
        
        reranked_chunks = state.get("reranked_chunks") or []
        if not reranked_chunks:
            print("  No reranked contexts, skipping reranking evaluation.")
            round_idx = state.get("rerank_round", 0) + 1
            return {"reranking_eval": None, "rerank_round": round_idx}
        
        contexts = [chunk['chunk_text'] for chunk in reranked_chunks]
        score = await self.evaluator.evaluate_reranking(
            query=state["query"],
            contexts=contexts,
            ground_truth=state.get("ground_truth")   
        )

        judge_update: Dict[str, Any] = {}
        if getattr(self.prompter, "judge", None) and getattr(self.prompter.judge, "enabled", False):
            judge_feedback = self.prompter.judge.assess_reranking(query=state["query"], contexts=contexts)
            judge_update = self._update_judge_eval(state, "reranking", judge_feedback)

        # Lightweight structural metrics to satisfy routing thresholds.
        scores = [chunk.get("cross_encoder_score", 0.0) for chunk in reranked_chunks if chunk is not None]
        score_concentration = 0.0
        if scores:
            sorted_scores = sorted(scores, reverse=True)
            total_scores = sum(sorted_scores) or 1e-9
            score_concentration = sum(sorted_scores[: min(3, len(sorted_scores))]) / total_scores

        baseline = (state.get("retrieval_eval") or {}).get("context_precision")
        rerank_precision = score.get("context_precision", 0.0)
        score_improvement = (rerank_precision - baseline) if baseline is not None else 0.0

        reranking_confidence = self.reward_manager._safe_avg([rerank_precision, score_concentration])  # type: ignore[attr-defined]
        score.update(
            {
                "score_concentration": score_concentration,
                "score_improvement": score_improvement,
                "reranking_confidence": reranking_confidence,
            }
        )
        
        round_idx = state.get("rerank_round", 0) + 1
        
        prompt_records, improvements = self._finalize_prompt_stage(
            stage="reranking",
            state_snapshot=state,
            metric_overrides={
                "reranking_eval": score,
                "context_precision_score": score.get('context_precision'),
                "score_concentration": score_concentration,
                "score_improvement": score_improvement,
                "reranking_confidence": reranking_confidence,
            }
        )
        
        return {
            "reranking_eval": score,
            "rerank_round": round_idx,
            "iteration": state.get("iteration", 0) + 1,
            "prompt_history": prompt_records or [],
            "improvement_history": improvements or [],
            **judge_update,
        }

    async def _evaluate_generation_node(self, state: RAGState) -> Dict[str, Any]:
        """Evaluate the generated answer using RAGAS generation metrics"""
        print(f"[Iteration {state['iteration']}] Evaluating generation output...")
        
        # Use RAGAS to evaluate generation (faithfulness, answer_relevancy, context_utilization)
        scores = await self.evaluator.evaluate_generation(
            query=state["query"],
            contexts=state.get("final_contexts") or [],
            answer=state.get("answer") or "",
            ground_truth=state.get("ground_truth")   
        )

        judge_update: Dict[str, Any] = {}
        if getattr(self.prompter, "judge", None) and getattr(self.prompter.judge, "enabled", False):
            judge_feedback = self.prompter.judge.assess_generation(
                query=state["query"],
                answer=state.get("answer") or "",
                contexts=state.get("final_contexts") or [],
                reference=state.get("ground_truth"),
            )
            judge_update = self._update_judge_eval(state, "generation", judge_feedback)
        faithfulness_score = scores.get('faithfulness') or 0.0
        answer_correctness_score = scores.get('answer_correctness') or 0.0
        answer_relevancy_score = scores.get('answer_relevancy') or answer_correctness_score
        available = [val for val in (faithfulness_score, answer_relevancy_score) if val is not None]
        overall_score = sum(available) / len(available) if available else 0.0
        
        round_idx = state.get("generation_round", 0) + 1
        
        prompt_records, improvements = self._finalize_prompt_stage(
            stage="generation",
            state_snapshot=state,
            metric_overrides={
                "generation_eval": scores,
                "faithfulness_score": faithfulness_score,
                "answer_correctness_score": answer_correctness_score,
                "answer_relevancy_score": answer_relevancy_score,
                "overall_score": overall_score,
            }
        )
        
        return {
            "generation_eval": scores,
            "generation_round": round_idx,
            "iteration": state.get("iteration", 0) + 1,
            "faithfulness_score": faithfulness_score,
            "answer_correctness_score": answer_correctness_score,
            "answer_relevancy_score": answer_relevancy_score,
            "overall_score": overall_score,
            "prompt_history": prompt_records or [],
            "improvement_history": improvements or [],
            **judge_update,
        }
    
    
    def _route_after_retrieval_eval(self, state: RAGState) -> str:
        """Decide whether to rerank or repeat reformulation based on RAGAS retrieval metrics"""
        # Thresholds for RAGAS retrieval metrics
        PRECISION_THRESHOLD = 0.60  # Context precision (relevant contexts ranked high)
        RECALL_THRESHOLD = 0.50     # Context recall (coverage of relevant info)
        COMBINED_THRESHOLD = 0.55   # Average of both

        rounds = state.get("retrieval_round", 0)
        max_rounds = state.get("max_iterations", self.max_iterations)
        metrics = state.get("retrieval_eval") or {}
        reward_signal = self._latest_reward(state, "reformulation") or 0.0
        
        # Get RAGAS retrieval metrics
        context_precision = metrics.get("context_precision", 0.0)
        context_recall = metrics.get("context_recall", 0.0)
        combined_score = (context_precision + context_recall) / 2

        if rounds >= max_rounds:
            print(f"✓ Max retrieval rounds ({rounds}/{max_rounds}); moving to rerank")
            return "rerank"
        
        # Primary check: combined retrieval quality
        if combined_score >= COMBINED_THRESHOLD or reward_signal >= COMBINED_THRESHOLD:
            print(f"✓ Retrieval acceptable (precision={context_precision:.3f}, "
                  f"recall={context_recall:.3f}, combined={combined_score:.3f}, reward={reward_signal:.3f}); continuing to rerank")
            return "rerank"
        
        # Check precision (relevant contexts ranked highly)
        if context_precision < PRECISION_THRESHOLD and reward_signal < PRECISION_THRESHOLD:
            print(f"↻ Context precision too low ({context_precision:.3f}); retrying reformulation")
            return "reformulate_query"
        
        # Check recall (coverage of relevant information)
        if context_recall < RECALL_THRESHOLD and reward_signal < RECALL_THRESHOLD:
            print(f"↻ Context recall too low ({context_recall:.3f}); retrying reformulation")
            return "reformulate_query"
        
        # Marginal but proceed
        print(f"⚠️  Retrieval marginal (combined={combined_score:.3f}); proceeding to rerank")
        return "rerank"
    
    def _route_after_reranking_eval(self, state: RAGState) -> str:
        """Decide whether to regenerate contexts or proceed to answer generation."""
        # Thresholds for reranking quality (using score-based metrics)
        RERANKING_CONFIDENCE_THRESHOLD = 0.70
        CONCENTRATION_MIN = 0.50  # How much better are top results
        IMPROVEMENT_MIN = 0.05  # Minimum improvement from reranking

        rounds = state.get("rerank_round", 0)
        max_rounds = state.get("max_iterations", self.max_iterations)
        metrics = state.get("reranking_eval") or {}
        reward_signal = self._latest_reward(state, "reranking") or 0.0

        # Get reranking quality metrics
        reranking_confidence = metrics.get("reranking_confidence", 0.0)
        score_concentration = metrics.get("score_concentration", 0.0)
        score_improvement = metrics.get("score_improvement", 0.0)

        if rounds >= max_rounds:
            print(f"✓ Max reranking rounds ({rounds}/{max_rounds}); moving to generation")
            return "generate"
        
        # Primary check: overall reranking confidence
        if reranking_confidence >= RERANKING_CONFIDENCE_THRESHOLD or reward_signal >= RERANKING_CONFIDENCE_THRESHOLD:
            print(f"✓ Reranking acceptable (confidence={reranking_confidence:.3f}, reward={reward_signal:.3f}); continuing to generation")
            return "generate"
        
        # Check if reranking actually improved things
        if score_improvement < IMPROVEMENT_MIN:
            print(f"↻ Reranking didn't improve scores ({score_improvement:.3f}); retrying")
            return "rerank"
        
        # Check if top results are clearly better
        if score_concentration < CONCENTRATION_MIN:
            print(f"↻ Top results not clearly better ({score_concentration:.3f}); retrying")
            return "rerank"
        
        # Marginal but proceed
        print(f"⚠️  Reranking marginal (confidence={reranking_confidence:.3f}); proceeding to generation")
        return "generate"
    
    def _route_after_generation_eval(self, state: RAGState) -> str:
        """Decide whether to regenerate answer based on RAGAS generation metrics"""
        # Thresholds for RAGAS generation metrics
        FAITHFULNESS_THRESHOLD = 0.75      # Answer grounded in context
        ANSWER_RELEVANCY_THRESHOLD = 0.70  # Answer relevance to query
        OVERALL_THRESHOLD = 0.70           # Combined threshold

        rounds = state.get("generation_round", 0)
        max_rounds = state.get("max_iterations", self.max_iterations)
        overall = state.get("overall_score") or 0.0
        reward_signal = self._latest_reward(state, "generation") or 0.0
        
        # Get individual RAGAS generation metrics
        faithfulness = state.get("faithfulness_score") or 0.0
        answer_relevancy = (
            state.get("answer_relevancy_score")
            or state.get("answer_correctness_score")
            or 0.0
        )

        if not overall:
            overall = (faithfulness + answer_relevancy) / 2 if (faithfulness or answer_relevancy) else 0.0
        
        if rounds >= max_rounds:
            print(f"✓ Generation reached max iterations ({rounds}/{max_rounds}); finishing")
            return END
        
        if overall >= OVERALL_THRESHOLD or reward_signal >= OVERALL_THRESHOLD:
            print(f"✓ Generation met quality threshold (faithfulness={faithfulness:.3f}, "
                  f"relevancy={answer_relevancy:.3f}, overall={overall:.3f}, reward={reward_signal:.3f}); finishing")
            return END
        
        # Check if faithfulness is critically low
        if faithfulness < FAITHFULNESS_THRESHOLD:
            print(f"↻ Faithfulness too low ({faithfulness:.3f}); regenerating")
            return "generate"
        
        # Check if answer relevancy is low
        if answer_relevancy < ANSWER_RELEVANCY_THRESHOLD:
            print(f"↻ Answer relevancy too low ({answer_relevancy:.3f}); regenerating")
            return "generate"
        
        print(f"⚠️  Generation below threshold (overall={overall:.3f}); finishing anyway")
        return END
    
    
    def _build_initial_state(
        self,
        query: str,
        ground_truth: Optional[str],
        retrieval_k: int,
        final_k: int,
        max_iterations: Optional[int],
        convergence_threshold: Optional[float],
    ) -> RAGState:
        """Helper to construct a fresh state for each graph run."""
        return {
            "query": query,
            "ground_truth": ground_truth,
            "retrieval_k": retrieval_k,
            "final_k": final_k,
            "max_iterations": max_iterations or self.max_iterations,
            "convergence_threshold": convergence_threshold or self.convergence_threshold,
            "iteration": 0,
            "converged": False,
            "improvement_history": [],
            # Optional fields initialized as None
            "reformulated_query": None,
            "reformation_rationale": None,
            "retrieved_chunks": None,
            "contexts": None,
            "retrieval_method": None,
            "reranked_chunks": None,
            "final_contexts": None,
            "reranking_rationale": None,
            "answer": None,
            "answer_reference": None,
            "generation_rationale": None,
            "faithfulness_score": None,
            "answer_relevancy_score": None,
            "context_utilization_score": None,
            "context_precision_score": None,
            "context_recall_score": None,
            "overall_score": None,
            "retrieval_eval": None,
            "reranking_eval": None,
            "generation_eval": None,
            "judge_eval": None,
            "prompt_history": [],
            "retrieval_round": 0,
            "rerank_round": 0,
            "generation_round": 0,
        }
    
    async def _run_compiled_graph(
        self,
        compiled_graph,
        initial_state: RAGState,
        thread_suffix: str = "",
    ) -> Dict[str, Any]:
        """Execute a compiled graph run and extract final outputs."""
        thread_id = f"query-{hash(initial_state['query'])}{thread_suffix}"
        config = {"configurable": {"thread_id": thread_id}}
        final_state = None

        print(f"\n{'='*80}")
        print(f"Processing query: {initial_state['query']}")
        print(f"{'='*80}")
        
        async for state_update in compiled_graph.astream(initial_state, config=config):
            final_state = state_update

        if final_state and isinstance(final_state, dict):
            last_node_key = list(final_state.keys())[-1]
            final_result = final_state[last_node_key]
            
            print(f"\n{'='*80}")
            print("FINAL RESULTS")
            print(f"{'='*80}")
            print(f"Iterations: {len(final_result.get('improvement_history', []))}")
            print(f"Final Answer: {final_result.get('answer', 'N/A')}")
            print(f"Overall Score: {(final_result.get('overall_score') or 0.0):.3f}")
            print(f"{'='*80}\n")
            
            return final_result
        
        return {}

    async def query(
        self,
        query: str,
        ground_truth: Optional[str] = None,
        retrieval_k: int = 20,
        final_k: int = 10,
        max_iterations: Optional[int] = None,
        convergence_threshold: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Process a query through the self-improving pipeline
        
        Args:
            query: User's question
            ground_truth: Optional ground truth answer for evaluation
            retrieval_k: Initial number of chunks to retrieve
            final_k: Initial number of chunks for generation
            max_iterations: Override default max iterations
            convergence_threshold: Override default convergence threshold
        
        Returns:
            Final state with answer and evaluation history
        """
        if not self.is_ready:
            raise ValueError("Pipeline not ready. Load documents first.")
        
        initial_state = self._build_initial_state(
            query=query,
            ground_truth=ground_truth,
            retrieval_k=retrieval_k,
            final_k=final_k,
            max_iterations=max_iterations,
            convergence_threshold=convergence_threshold,
        )
        
        checkpointer = MemorySaver()
        compiled_graph = self.graph.compile(checkpointer=checkpointer)
        
        return await self._run_compiled_graph(
            compiled_graph=compiled_graph,
            initial_state=initial_state,
            thread_suffix=""
        )
    
    def query_sync(
        self,
        query: str,
        ground_truth: Optional[str] = None,
        retrieval_k: int = 20,
        final_k: int = 10,
        max_iterations: Optional[int] = None,
        convergence_threshold: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Synchronous wrapper for query() - useful for notebooks and scripts
        
        Args:
            Same as query()
        
        Returns:
            Final state with answer and evaluation history
        """
        return asyncio.run(self.query(
            query=query,
            ground_truth=ground_truth,
            retrieval_k=retrieval_k,
            final_k=final_k,
            max_iterations=max_iterations,
            convergence_threshold=convergence_threshold
        ))

    async def query_batch(
        self,
        queries: List[Union[str, Dict[str, Any]]],
        retrieval_k: int = 20,
        final_k: int = 10,
        max_iterations: Optional[int] = None,
        convergence_threshold: Optional[float] = None,
        concurrency: int = 1,
        refresh_prompts: bool = False,
        refresh_threshold: float = 0.6,
        refresh_window: int = 5,
        refresh_metric_fns: Optional[Dict[str, Callable[[Any, Any], float]]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Process multiple queries with optional limited concurrency.
        
        Args:
            queries: List of query strings or dicts with keys 'query' and optional 'ground_truth'.
            retrieval_k: Initial number of chunks to retrieve for each query.
            final_k: Initial number of chunks for generation for each query.
            max_iterations: Override default max iterations.
            convergence_threshold: Override default convergence threshold.
            concurrency: Max number of simultaneous graph runs (sharing the same prompt optimizer).
            refresh_prompts: If True, refresh DSPy prompts between batches using observed rewards.
            refresh_threshold: Mean reward threshold below which a refresh is triggered.
            refresh_window: Number of recent rewards to consider for the refresh trigger.
            refresh_metric_fns: Optional metric overrides for the DSPy optimizer during refresh.
        
        Returns:
            List of final states in the same order as the input queries.
        """
        if not self.is_ready:
            raise ValueError("Pipeline not ready. Load documents first.")
        if concurrency < 1:
            raise ValueError("concurrency must be at least 1")
        if not queries:
            return []

        checkpointer = MemorySaver()
        compiled_graph = self.graph.compile(checkpointer=checkpointer)
        semaphore = asyncio.Semaphore(concurrency)
        results: List[Dict[str, Any]] = [{} for _ in queries]

        async def _run_single(idx: int, item: Union[str, Dict[str, Any]]) -> None:
            async with semaphore:
                if isinstance(item, dict):
                    query_text = item.get("query")
                    ground_truth_val = item.get("ground_truth")
                else:
                    query_text = str(item)
                    ground_truth_val = None

                if not query_text:
                    results[idx] = {}
                    return

                initial_state = self._build_initial_state(
                    query=query_text,
                    ground_truth=ground_truth_val,
                    retrieval_k=retrieval_k,
                    final_k=final_k,
                    max_iterations=max_iterations,
                    convergence_threshold=convergence_threshold,
                )

                results[idx] = await self._run_compiled_graph(
                    compiled_graph=compiled_graph,
                    initial_state=initial_state,
                    thread_suffix=f"-batch-{idx}"
                )

        await asyncio.gather(*[
            _run_single(idx, item) for idx, item in enumerate(queries)
        ])

        if refresh_prompts:
            self._refresh_prompts_from_results(
                results,
                threshold=refresh_threshold,
                window=refresh_window,
                metric_fns=refresh_metric_fns,
            )

        return results

    def query_batch_sync(
        self,
        queries: List[Union[str, Dict[str, Any]]],
        retrieval_k: int = 20,
        final_k: int = 10,
        max_iterations: Optional[int] = None,
        convergence_threshold: Optional[float] = None,
        concurrency: int = 1,
        refresh_prompts: bool = False,
        refresh_threshold: float = 0.6,
        refresh_window: int = 5,
        refresh_metric_fns: Optional[Dict[str, Callable[[Any, Any], float]]] = None,
    ) -> List[Dict[str, Any]]:
        """Synchronous wrapper for query_batch()."""
        return asyncio.run(self.query_batch(
            queries=queries,
            retrieval_k=retrieval_k,
            final_k=final_k,
            max_iterations=max_iterations,
            convergence_threshold=convergence_threshold,
            concurrency=concurrency,
            refresh_prompts=refresh_prompts,
            refresh_threshold=refresh_threshold,
            refresh_window=refresh_window,
            refresh_metric_fns=refresh_metric_fns,
        ))
